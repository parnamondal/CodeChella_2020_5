# -*- coding: utf-8 -*-
"""CodeChella.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sFVVNXmt0vCTsunpNZgLWjEPX5XimIzp
"""

pip install turicreate

import turicreate as tc

train = tc.SFrame.read_csv('train.csv')

test = tc.SFrame.read_csv('test.csv')

train.head()

#Convert Tweets to Lower Case
def lower_case(df):
    df['tweet'] = df['tweet'].apply(lambda x: " ".join(x.lower() for x in x.split()))

lower_case(train)
lower_case(test)

train.head()

#Remove punctuation from the Tweets
def punctuation_removal(df):
    df['tweet'] = df['tweet'].apply(lambda x: re.sub('[^\w\s]','',x))

import re

punctuation_removal(train)
punctuation_removal(test)

#importing NLTK library
import nltk
nltk.download('stopwords')

#Stopwords 
from nltk.corpus import stopwords
stop = stopwords.words('english')

#Remove Stop-Words from Tweet
def stop_words_removal(df):
    df['tweet'] = df['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

stop_words_removal(train)
stop_words_removal(test)

import pandas as pd

freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]
freq

freq = list(freq.index)
freq

#Remove frequently occuring words
def frequent_words_removal(df):    
    df['tweet'] = df['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in freq))

frequent_words_removal(train)
frequent_words_removal(test)

#Rarely occuring words
rare = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]
rare

#Removing Rarely occuring words
def rare_words_removal(df):
    df['tweet'] = df['tweet'].apply(lambda x: " ".join(x for x in x.split() if x not in rare))

rare_words_removal(train)
rare_words_removal(test)

import nltk
nltk.download('wordnet')

from textblob import Word

def lemmatization(df):
    df['tweet'] = df['tweet'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))
    print(df['tweet'].head())

lemmatization(train)
lemmatization(test)

train.head()

train_data,test_data = train.random_split(.8,seed=0)

train_data['word_count'] = tc.text_analytics.count_words(train_data['tweet'])

train_data.head()

test_data['word_count'] = tc.text_analytics.count_words(test_data['tweet'])

test_data.head()

test['word_count'] = tc.text_analytics.count_words(test['tweet'])

sentiment_model = tc.logistic_classifier.create(train_data,
                                               target='label', 
                                               features=['word_count'],
                                                validation_set=test_data)

sentiment_model.validation_accuracy

sentiment_model.validation_precision

sentiment_model.validation_confusion_matrix

test_data['predicted_sentiment'] = sentiment_model.predict(test_data, output_type = 'probability')

test_data.head()

test_data = test_data.sort('predicted_sentiment', ascending=False)

test_data.head()

train['predicted_sentiment'] = sentiment_model.predict(train, output_type = 'probability')

train.head()

train = train.sort('label', ascending=False)

def new_label(df):
  if df['predicted_sentiment']>=0.9:
    return 1
  else:
    return 0
  print(df)

test_data['new_label'] = test_data.apply(lambda x: new_label(x))

test_data.head()

tc.evaluation.accuracy(test_data['label'], test_data['new_label'], average = 'micro')